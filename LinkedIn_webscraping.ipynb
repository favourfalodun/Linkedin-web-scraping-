{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup \n",
    "from pyparsing import line\n",
    "import time\n",
    "import csv\n",
    "\n",
    "\n",
    "# intialize chrome service\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"\n",
    "driver_path = r\"C:\\chromedriver.exe\"\n",
    "chrome_service = Service(driver_path)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "browser = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
    "\n",
    "\n",
    "#read the login credentials\n",
    "login =open('login_credential.txt')\n",
    "line = login.readlines()\n",
    "email = line[0]\n",
    "pwd = line[1]\n",
    "searchQuery =line[2]\n",
    "input_pages = int(line[3])\n",
    "\n",
    "# navigate to a website\n",
    "browser.get('https://www.linkedin.com/login')\n",
    "time.sleep(3)\n",
    "\n",
    "#input the email into the website\n",
    "email_field = browser.find_element(by=By.ID, value =\"username\")\n",
    "email_field.send_keys(email)\n",
    "time.sleep(3)\n",
    "\n",
    "#input the password into the website\n",
    "password_field = browser.find_element(by=By.ID, value ='password')\n",
    "password_field.send_keys(pwd)\n",
    "time.sleep(5)\n",
    "\n",
    "#click the send button\n",
    "#loginbutton = browser.find_element(by=By.CLASS_NAME, value=\"btn__primary--large from__button--floating\")\n",
    "#loginbutton.click()\n",
    "#time.sleep(10)\n",
    "\n",
    "#input the search query\n",
    "searchbutton = browser.find_element(by=By.XPATH, value ='//*[@id=\"global-nav-typeahead\"]/input')\n",
    "searchbutton.send_keys(searchQuery)\n",
    "time.sleep(3)\n",
    "\n",
    "#search\n",
    "searchbutton.send_keys(Keys.RETURN)\n",
    "time.sleep(3)\n",
    "\n",
    "#PeopleResult = browser.find_element(by=By.XPATH, value ='//*[@id=\"search-reusables__filters-bar\"]/ul/li[2]/button')\n",
    "#PeopleResult.click()\n",
    "#time.sleep(3)\n",
    "\n",
    "\n",
    "#function to extract profile url link from the different profile displayed\n",
    "def GetURL():\n",
    "    all_profile_URL=[]\n",
    "    pagesource = BeautifulSoup(browser.page_source)\n",
    "    profiles = pagesource.find_all('a', class_ = 'app-aware-link')\n",
    "    for profile in profiles:\n",
    "        profile_URL = profile.get('href')\n",
    "        if 'https://www.linkedin.com/in' in profile_URL and  profile_URL not in all_profile_URL:\n",
    "            all_profile_URL.append(profile_URL)\n",
    "    return all_profile_URL\n",
    "    \n",
    "#loop through the number of pages to get the url for the different profile\n",
    "def GetURLonPage():\n",
    "    UrLs_all_page = []\n",
    "    for page in range(input_pages):\n",
    "        URLs_one_page = GetURL()\n",
    "        time.sleep(3)\n",
    "        browser.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "        time.sleep(2)\n",
    "        nextbutton = browser.find_element(by=By.CLASS_NAME, value=\"artdeco-pagination__button--next\")\n",
    "        nextbutton.click()\n",
    "        UrLs_all_page = UrLs_all_page + URLs_one_page\n",
    "        time.sleep(3)\n",
    "    return UrLs_all_page\n",
    "\n",
    "Urls_all_page = GetURLonPage()\n",
    "#create a csv file to store the relevant information\n",
    "with open('output.csv', 'w', newline='') as file_output:\n",
    "    headers = ['Name','Title','Location','URL']\n",
    "    writer = csv.DictWriter(file_output, delimiter=',' ,lineterminator=\"\\n\", fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "#loop the profile link to extract the relevant information\n",
    "    for linkedin_URL in Urls_all_page:\n",
    "        browser.get(linkedin_URL)\n",
    "        time.sleep(2)\n",
    "        page_source = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "        info_div = page_source.find('div', class_=\"mt2 relative\")\n",
    "        name = info_div.find(\"h1\", class_ ='text-heading-xlarge inline t-24 v-align-middle break-words').get_text().strip()\n",
    "        title_name = info_div.find(\"div\", class_ ='text-body-medium break-words').get_text().strip()\n",
    "        city_name = info_div.find(\"span\", class_=\"text-body-small inline t-black--light break-words\").get_text().strip()\n",
    "        writer.writerow({headers[0]:name, headers[1]:title_name, headers[2]:city_name,headers[3]:linkedin_URL})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c4af27564e8e4e69140fe91917a9025929ff2059b980474eafd97c8af206d8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
